[**ä¸­æ–‡**](./README.md) | [**English**](./README_EN.md)
<p align="center" width="100%">
<a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/" target="_blank"><img src="assets/logo/logo_new.png" alt="SCIR-HI-HuaTuo" style="width: 60%; min-width: 300px; display: block; margin: auto;"></a>
</p>
# BenTsao (original name: HuaTuo): Instruction-tuning Large Language Models With Chinese Medical Knowledge

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/blob/main/LICENSE) [![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

This repo open-sources a series of instruction-tuned large language models with Chinese medical instruction datasets, including LLaMAã€Alpaca-Chineseã€Bloomã€Huozi. 

We constructed a Chinese medical instruct-tuning dataset using medical knowledge graphs, medical literatures and the GPT3.5 API, and performed instruction-tuning on various base models with this dataset, improving its question-answering performance in the medical field.


## News
**[2023/08/07] ğŸ”¥ğŸ”¥ Released a model instruction-tuned based on [Huozi](https://github.com/HIT-SCIR/huozi), resulting in a significant improvement in model performance. ğŸ”¥ğŸ”¥**

[2023/08/05] The "Bentsao" model was presented as a poster at the CCL 2023 Demo Track.

[2023/08/03] SCIR Lab open-sourced the [Huozi](https://github.com/HIT-SCIR/huozi) general question-answering model. Everyone is welcome to check it out! ğŸ‰ğŸ‰

[2023/07/19] Released a model instruction-tuned based on [Bloom](https://huggingface.co/bigscience/bloom-7b1).

[2023/05/12] The model was renamed from "Huatuo" to "Bentsao".

[2023/04/28] Released a model instruction-tuned based on the [Chinese-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca).

[2023/04/24] Released a model instruction-tuned based on LLaMA with medical literature.

[2023/03/31] Released a model instruction-tuned based on LLaMA with a medical knowledge base.







## A Quick Start
Firstly, install the required packages. It is recommended to use Python 3.9 or above

```
pip install -r requirements.txt
```

For all base models, we adopted the semi-precision base model LoRA fine-tuning method for instruction fine-tuning training, in order to strike a balance between computational resources and model performance.

### Base models
 - [Huozi1.0](https://github.com/HIT-SCIR/huozi), Bloom-7B-based Chinese QA model
 - [Bloom-7B](https://huggingface.co/bigscience/bloomz-7b1)
 - [Alpaca-Chinese-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca), LLaMA-based Chinese QA model
 - [LLaMA-7B](https://huggingface.co/decapoda-research/llama-7b-hf)


### LoRA weight download
LORA weights can be downloaded through Baidu Netdisk or Huggingface.

1. ğŸ”¥LoRA for Huozi 1.0
  - with the medical knowledge base and medical QA dataset [BaiduDisk](https://pan.baidu.com/s/1BPnDNb1wQZTWy_Be6MfcnA?pwd=m21s)
2. LoRA for Bloom
 - with the medical knowledge base and medical QA dataset [BaiduDisk](https://pan.baidu.com/s/1jPcuEOhesFGYpzJ7U52Fag?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-bloom-med-bloom)
3. LoRA for Chinese Alpaca
 - with the medical knowledge base [BaiduDisk](https://pan.baidu.com/s/16oxcjzXnXjDpL8SKihgNxw?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med)
 - with the medical knowledge base and medical literature [BaiduDisk](https://pan.baidu.com/s/1HDdK84ASHmzOFlkmypBIJw?pwd=scir) and [Hugging Face](https://huggingface.co/lovepon/lora-alpaca-med-alldata)
4. LoRA for LLaMA
 - with the medical knowledge base [BaiduDisk](https://pan.baidu.com/s/1jih-pEr6jzEa6n2u6sUMOg?pwd=jjpf) and [Hugging Face](https://huggingface.co/thinksoso/lora-llama-med)
 - with medical literature [BaiduDisk](https://pan.baidu.com/s/1jADypClR2bLyXItuFfSjPA?pwd=odsk) and [Hugging Face](https://huggingface.co/lovepon/lora-llama-literature)


Download the LORA weight file and extract it. The format of the extracted file should be as follows:

```
**lora-folder-name**/
Â  - adapter_config.json Â  # LoRA configuration
Â  - adapter_model.bin Â  # LoRA weight
```

We also trained a medical version of ChatGLM: [ChatGLM-6B-Med](https://github.com/SCIR-HI/Med-ChatGLM) based on the same data.


### Infer
We provided some test cases in `./data/infer.json`, which can be replaced with other datasets. Please make sure to keep the format consistent.

Run the infer script

```
#Based on medical knowledge base
bash ./scripts/infer.sh

#Based on medical literature
#single-epoch
bash ./scripts/infer-literature-single.sh

#multi-epoch
bash ./scripts/infer-literature-multi.sh
```

The code for the infer.sh script is as follows. Please replace the base model base_model, the LoRA weights lora_weights, and the test dataset path instruct_dir before running

	python infer.py \
			    --base_model 'BASE_MODEL_PATH' \
			    --lora_weights 'LORA_WEIGHTS_PATH' \
			    --use_lora True \
			    --instruct_dir 'INFER_DATA_PATH' \
			    --prompt_template 'TEMPLATE_PATH'
		    

The prompt template is relevant to the model as follows: 

| Huozi&Bloom                      | LLaMA&Alpaca                                                                          |                                       
|:------------------------------|:--------------------------------------------------------------------------------------|
| `templates/bloom_deploy.json` | with the medical knowledge base`templates/med_template.json` <br>  with the medical literature`templates/literature_template.json` |



other reference in `./scripts/test.sh`

	
## Methodology
The base model has limited effectiveness in medical question-answering scenarios. Instruction-tuning is an efficient method to give the base model the ability to answer human questions
	    
### Dataset construction
#### Medical knowledge base
We used both publicly available and self-built Chinese medical knowledge bases, with a primary reference to [cMeKG](https://github.com/king-yyf/CMeKG_tools). The medical knowledge base is built around diseases, drugs, and diagnostic indicators, with fields including complications, risk factors, histological examinations, clinical symptoms, drug treatments, and adjuvant therapies. An example of the knowledge base is shown below:


```
{"ä¸­å¿ƒè¯": "åå¤´ç—›", "ç›¸å…³ç–¾ç—…": ["å¦Šå¨ åˆå¹¶åå¤´ç—›", "æ¶å¯’å‘çƒ­"], "ç›¸å…³ç—‡çŠ¶": ["çš®è‚¤å˜ç¡¬", "å¤´éƒ¨åŠçœ¼åéƒ¨ç–¼ç—›å¹¶èƒ½å¬åˆ°è¿ç»­ä¸æ–­çš„éš†éš†å£°", "æ™¨èµ·å¤´ç—›åŠ é‡"], "æ‰€å±ç§‘å®¤": ["ä¸­è¥¿åŒ»ç»“åˆç§‘", "å†…ç§‘"], "å‘ç—…éƒ¨ä½": ["å¤´éƒ¨"]}
```
We used the GPT3.5 API to build a Question-Answer dataset around a medical knowledge base, with multiple prompts to fully leverage the knowledge.

An example of the training dataset for instruct-tuning is shown below:


```
"é—®é¢˜ï¼šä¸€ä½å¹´è½»ç”·æ€§é•¿æœŸä½¿ç”¨å¯å¡å› ï¼Œçªç„¶å‡ºç°èƒ¸ç—›ã€å‘•åã€å‡ºæ±—ç­‰ç—‡çŠ¶ï¼Œç»æ£€æŸ¥å‘ç°å¿ƒç”µå›¾åæ˜ å¿ƒè‚Œæ€¥æ€§æŸä¼¤ï¼Œè¯·é—®å¯èƒ½æ‚£çš„æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿæ²»ç–—æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ"
å›ç­”: å¯èƒ½æ‚£çš„æ˜¯å¿ƒè‚Œæ¢—å¡ï¼Œéœ€è¦è¿›è¡Œç»´æ‹‰å¸•ç±³ã€ä¾æ™®åˆ©é…®ã€ç¡é…¸ç”˜æ²¹ã€ÃŸé˜»æ»å‰‚ã€å—å•¡ç­‰è¯ç‰©æ²»ç–—ï¼Œå¹¶è¿›è¡Œæº¶æ “æ²»ç–—ã€ä½åˆ†å­é‡è‚ç´ ã€é’™é€šé“é˜»æ»å‰‚ç­‰è¾…åŠ©æ²»ç–—ã€‚æ­¤å¤–éœ€è¦åŠæ—¶åœç”¨å¯å¡å› ç­‰è¯ç‰©ï¼Œä»¥é˜²æ­¢ç—…æƒ…åŠ é‡ã€‚"
```

We provided a training dataset for the model, consisting of more than eight thousand entries. It should be noted that although knowledge has been incorporated into the construction of the training set, there are still errors and imperfections. We will use better strategies to iteratively update the dataset in the future.

The quality of the dataset for instruct-tuning is still limited. We will continue to iterate and improve it. Meanwhile, the medical knowledge base and dataset construction code are still being organized and will be released once completed.

#### Medical literature
In addition, we collected Chinese medical literature on liver cancer in 2023, and used the GPT3.5 interface to collect multiple rounds of question-and-answer data around the medical literature. We provide 1k training examples in `./data_literature/liver_cancer.json`. At present, the quality of training samples is still limited. In the future, we will further iterate the data and release it in the form of `public dataset`. An example of a training sample is as follows:

<p align="center" width="100%">

<a href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/" target="_blank"><img src="assets/case.png" alt="SCIR-HI-HuaTuo-literature" style="width: 100%; min-width: 300px; display: block; margin: auto;"></a>

</p>

At present, we only open model parameters trained for the single disease "liver cancer". In the future, we plan to release a medical dialogue dataset incorporating medical literature conclusions, and plan to train models for 16 diseases related to "liver, gallbladder and pancreas".

### Finetune
To fine-tune LLaMA with your own dataset, please construct your dataset following the format of `./data/llama_data.json` and run the finetune script.

```
bash ./scripts/finetune.sh
```



## Training details
### GPU resource
We trained the model on an A100-SXM-80GB for a total of 10 epochs, which took approximately 2 hours and 17 minutes. With a batch size of 128, the GPU memory usage was around 40GB. We expect that GPU with 24GB or more of memory, such as the 3090/4090, will be able to support the model well, and the batch size can be adjusted based on the size of the GPU memory.


### Experiments
Parameter settings and details training process can be found in the wandb log:
https://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso

## Comparison of models

Note: test in March 2023.

| Inputå…¥ | Llama output  | Alpaca output | HuaTuo output |
|:------------- |:---------------| :-------------|:-------------|
|å°å¼ æœ€è¿‘æ„Ÿè§‰èº«ä½“ä¸é€‚ï¼Œå‡ºç°å¿ƒæ‚¸ã€æ°”ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ£€å‘ç°å¿ƒè„æ‰©å¤§ã€æåŠ¨å‡å¼±ã€‚|é—®é¢˜ï¼šå°å¼ æœ€è¿‘æ„Ÿè§‰èº«ä½“ä¸é€‚ï¼Œå‡ºç°å¿ƒæ‚¸ã€æ°”ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ£€å‘ç°å¿ƒè„æ‰©å¤§ã€æåŠ¨å‡å¼±ã€‚| æœ€è¿‘ï¼Œå°å¼ ä¸€ç›´æ„Ÿè§‰èº«ä½“ä¸èˆ’æœï¼Œæœ‰å¿ƒçƒ¦æ„ä¹±ã€å‘¼å¸æ€¥ä¿ƒç­‰ç—‡çŠ¶ã€‚ä½“æ ¼æ£€æŸ¥æ˜¾ç¤ºå¿ƒè„å¢å¤§ï¼Œè‚Œè‚‰æ— åŠ›ã€‚|**å°å¼ å¯èƒ½æ‚£æœ‰å¿ƒè‚Œç‚ï¼Œå»ºè®®å°½å¿«åˆ°åŒ»é™¢è¿›è¡Œæ£€æŸ¥å’Œæ²»ç–—ã€‚æ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬è¯ç‰©æ²»ç–—å’Œæ‰‹æœ¯æ²»ç–—ã€‚**|
|éº»é£ç—…å’Œå„¿ç«¥å“®å–˜çš„ç—…å› æ˜¯å¦ä¸€è‡´ï¼Ÿ|å„¿ç«¥å“®å–˜å’Œéº»é£ç—…çš„ç—…å› æ˜¯ä¸€è‡´ï¼Œä½†æ˜¯å„¿ç«¥å“®å–˜å’Œéº»é£ç—…çš„ç—…å› ä¸ä¸€è‡´|æ˜¯çš„ï¼Œå“®å–˜å’Œå„¿ç«¥å–˜æ¯æœ‰ç›¸ä¼¼çš„åŸå› ã€‚| **ä¸ä¸€è‡´ï¼Œéº»é£ç—…çš„ç—…å› æ˜¯éº»é£æ†èŒæ„ŸæŸ“ï¼Œè€Œå„¿ç«¥å“®å–˜çš„ç—…å› æ˜¯å–‚å…»ä¸è‰¯ã€å¸çƒŸç­‰å› ç´ ã€‚**|
|ä¸€ä½æ‚£è€…å‡ºç°äº†æŒç»­æ€§çš„èƒ€ç—›å’Œé»„ç–¸ï¼Œç»å½±åƒå­¦æ£€æŸ¥åç¡®è¯Šä¸ºè‚èƒ†ç®¡ç»“çŸ³ç—…ï¼Œåº”è¯¥é‡‡å–å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿ|è‚èƒ†ç®¡ç»“çŸ³ç—…æ‚£è€…åº”è¯¥é‡‡å–å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿ|æ‚£è€…åº”æ¥å—æ‰‹æœ¯å’ŒåŒ–ç–—ç›¸ç»“åˆçš„æ²»ç–—ã€‚| **é’ˆå¯¹è‚èƒ†ç®¡ç»“çŸ³ç—…ï¼Œæ²»ç–—æ–¹æ¡ˆåŒ…æ‹¬æ‰‹æœ¯æ²»ç–—ã€è¯ç‰©æ²»ç–—å’Œè¾…åŠ©æ²»ç–—ç­‰ã€‚å…·ä½“æ²»ç–—æ–¹æ¡ˆéœ€è¦æ ¹æ®æ‚£è€…çš„å…·ä½“æƒ…å†µè€Œå®šã€‚**|

##FAQs

1. 	Q: Why was it renamed to "Bentsao"?
	
	A: For naming consistency with the SCIR large language model series, the name of the Chinese medical model has been renamed to "Bentsao".

2. 	Q: Why "Huatuo(åé©¼)" instead of "Huatuo(åä½—)"?
	
	A: We chose the name "Huatuo(åé©¼)" because our base model, LLaMA(ç¾æ´²é©¼), and Alpaca(ç¾Šé©¼). Inspired by their names and a pun on the name "tuoé©¼" & "tuoä½—", we named our model "Huatuo(åé©¼)".

3. 	Q: Does it use Chinese traditional medicine theories or data?
	
	A: Not at the moment.

4.	Q: The results from the model vary and are limited in effectiveness.

	A: Due to the diversity considerations of the generative model, results may vary with multiple runs. The current open-source model, owing to the limited Chinese corpus in LLaMA and Alpaca, and a rather coarse way of knowledge integration, may yield inconsistent results. Please try the bloom-based and character-based models.
5. Q: The model cannot run or the inferred content is completely unacceptable.
	
	A: Please ensure that you've installed the dependencies from the requirements, set up the CUDA environment and added the environment variables, and correctly input the downloaded model and Lora's storage location. Any repetition or partial mistakes in the inferred content are occasional issues with the llama-based model and relate to LLaMA's ability in Chinese, training data scale, and hyperparameter settings. Please try the new character-based model. If there are severe issues, please describe in detail the filename, model name, Lora configuration, etc., in an issue. Thank you all.
Q: Among the released models, which one is the best?
A: Based on our experience, the character-based model seems to perform relatively better.






## Contributors

This project was founded by the Health Intelligence Group of the Research Center for Social Computing and Information Retrieval at Harbin Institute of Technology, including [Haochun Wang](https://github.com/s65b40), [Yanrui Du](https://github.com/DYR1), [Chi Liu](https://github.com/thinksoso), [Rui Bai](https://github.com/RuiBai1999), [Nuwa Xi](https://github.com/rootnx), [Yuhan Chen](https://github.com/Imsovegetable), [Zewen Qiang](https://github.com/1278882181), [Jianyu Chen](https://github.com/JianyuChen01), [Zijian Li](https://github.com/FlowolfzzZ) supervised by Associate Professor [Sendong Zhao](http://homepage.hit.edu.cn/stanzhao?lang=zh), Professor Bing Qin, and Professor Ting Liu.


## Acknowledgements

This project has referred the following open-source projects. We would like to express our gratitude to the developers and researchers involved in those projects.

- Facebook LLaMA: https://github.com/facebookresearch/llama
- Stanford Alpaca: https://github.com/tatsu-lab/stanford_alpaca
- alpaca-lora by @tloen: https://github.com/tloen/alpaca-lora
- CMeKG https://github.com/king-yyf/CMeKG_tools
- æ–‡å¿ƒä¸€è¨€ https://yiyan.baidu.com/welcome The logo of this project is automatically generated by Wenxin Yiyan.

## Disclaimer
The resources related to this project are for academic research purposes only and strictly prohibited for commercial use. When using portions of third-party code, please strictly comply with the corresponding open source licenses. The content generated by the model is influenced by factors such as model computation, randomness, and quantization accuracy loss, and this project cannot guarantee its accuracy. The vast majority of the dataset used in this project is generated by the model, and even if it conforms to certain medical facts, it cannot be used as the basis for actual medical diagnosis. This project does not assume any legal liability for the content output by the model, nor is it responsible for any losses that may be incurred as a result of using the related resources and output results.


## Citation
If you use the data or code from this project, please declare the reference:

```
@misc{wang2023huatuo,
      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge}, 
      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},
      year={2023},
      eprint={2304.06975},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



